\section{Adversarial Attacks}\label{sec:adversarial_attacks}

Many machine learning models, including neural networks, consistently
misclassify adversarial examples - inputs formed by applying small but
intentionally worst-case perturbations to examples from the dataset, such that
the perturbed input results in the model outputting an incorrect answer with
high confidence \cite{goodfellow2014explaining}. An adversarial example is
described as

\begin{equation}
    \vect{x}^{adv}  = \vect{x} + \vect{\eta}.
\end{equation}

Where $\vect{x}^{adv}$ is the adversarial example, $\vect{x}$ the original input and $\eta$
the perturbation.


First, this section describes the most prominent attacks to generate
adversarial examples and then highlights efforts that have been made to connect
adversarial examples with autoencoders. The attacks mentioned here can be found
in the
cleverhans\footnote{\href{https://github.com/tensorflow/cleverhans}{github.com/tensorflow/cleverhans}}
library. Contributions to the library in relation with this paper can be found
here\footnote{\href{http://github.com/tensorflow/cleverhans/pull/1037}{github.com/tensorflow/cleverhans/pull/1037}}
and
here\footnote{\href{http://github.com/tensorflow/cleverhans/pull/1047}{github.com/tensorflow/cleverhans/pull/1047}}.


\subsection{Fast Gradient Sign Method Attack}
The "fast gradient sign method" (FGSM) refers to an attack that computes the
perturbation $\eta$ by calculating:

\begin{equation}\label{eq:fgsm}
    \vect{\eta} = \epsilon * sign(\nabla_x J (\vect{\theta}, \vect{x}, y)).
\end{equation}

Where $\vect{\theta}$ are the parameters of a model, $\vect{x}$ the input to the
model, $y$ the targets associated with $\vect{x}$ (for machine learning tasks
that have targets) and $J(\vect{\theta}, \vect{x}, y)$ the loss used to train
the neural network. Equation \ref{eq:fgsm} linearizes the cost function around
the current value of $\vect{\theta}$, obtaining an optimal max-norm constrained
pertubation $\vect{\eta}$. The elements of $\vect{\eta}$ are equal to the sign of
the elements of the gradient of the loss function with respect to the input.
$\epsilon$ is a number big enough such that the adversarial example is
effective, but small enough such that it won't be noticed in the generated
example. Figure \ref{fig:fgsm_panda} \cite{goodfellow2014explaining} visualizes
the effect of FGSM.


\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{/3/fgsm_panda}
    \caption{FGSM applied to GoogLeNet \cite{szegedy2015going} on ImageNet. By
    adding an inconspicuous pertubation to the original image, GoogLeNet's
    classification of the image can be changed.} 
	\label{fig:fgsm_panda}
\end{figure}

\subsection{Projected Gradient Descent Attack}
A straightforward way to enhance FGSM is, to apply it multiple times with a
small step size, and clip pixel values of intermediate results after each step
to make sure that they are in an $\epsilon-neighbourhood$  of the original
image:


\begin{equation}\label{eq:pgd}
    \vect{x}^{adv}_{0} = \vect{x}, \quad \vect{x}^{adv}_{N + 1}=Clip_{x, \epsilon} \Big \{\vect{x}^{adv}_{N} + \alpha sign\big(\nabla_{x}J(\vect{x}^{adv}_{N}, y)\big)  \Big\}
\end{equation}

Typcially $\alpha = 1$ is used. This means that the value of each pixel is only
changed by 1 on each step. This method is known as "projected gradient descent"
(PGD) or "basic iterative method". \cite{kurakin2016adversarial, madry2017towards}

\subsection{CarliniWagnerL2 attack}
The attack proposed by Carlini \& Wagner \cite{carlini2017towards} is considered
as one of the most powerful attacks. Their approach relies on the initial
formulation of adversarial examples \cite{szegedy2013intriguing} and formally 
defines the problem of finding an adversarial instance for an image x as follows:

\begin{align}
    & \text{minimize} \; \mathcal{D}(x, x + \delta) \\
    & \begin{aligned}
        \text{such that}  \;    & C(x + \delta) = t \\
                                & x + \delta \in [0, 1]^n 
    \end{aligned}
\end{align} 

where x is fixed, and the goal is to find $\delta$ that minimizes
$\mathcal{D}(x, x + \delta)$. That is, finding some small change $\delta$ that
when applied to $x$ will change its classification $C$ to target $t$ (first
constraint), but so that the result is still a valid image (second constraint).
$\mathcal{D}$ describes some distance metric. Applying gradient descent to this
optimization problem fails completely, because of the highly non-linear
constraint $C(x + \delta) = t$.

The authors solve this problem by reformulating it as a appropriate optimization
instance that can be solved by existing optimization algorithms. Therefore, they
express the above formulation in a different form that is better suited for
optimization:

\begin{align}
    & \text{minimize} \; \mathcal{D}(x, x + \delta) + f(x + \delta) \\
    & \begin{aligned}
        \text{such that}  \; & x + \delta \in [0, 1]^n  
    \end{aligned}
\end{align}


where $f(x + \delta)$ is some kind of loss function on how close $C(x + \delta)$
is to target $t$ with the property that $f(x + \delta) <= 0$ if and only if $C(x + \delta) =
t$. As an example lets consider the following loss function:

\begin{equation}
    f(x + \delta) = (1 - C(x + \delta)_{t})
\end{equation}

where $C(x + \delta)_{t}$ is the probability of $x + \delta$ being classified to the target
class $t$ with the classifier $C$. If $C(x + \delta)_{t}=1$ then the loss function
evalutes to:

\begin{equation}
    f(x + \delta) = (1 - C(x + \delta)_{t}) = (1 - 1) = 0
\end{equation}

If $C(x + \delta)_{t}=0$ then the loss function evaluates to:

\begin{equation}
    f(x + \delta) = (1 - C(x + \delta)_{t}) = (1 - 0) = 1
\end{equation}

The Carlini \& Wagner attack defeats defensive distillation
\cite{papernot2016distillation} and should be considered for evaluating the
adversarial robustness of deep learning models \cite{carlini2019onrobustness}.