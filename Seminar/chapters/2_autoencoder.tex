\section{Autoencoder}\label{sec:autoencoder}
This section starts with describing the basic philosophy of an autoencoder and
then takes a look at different types of autoencoders.

An autoencoder is a neural network trained to copy its input to its output
\cite{Goodfellow-et-al-2016}. It has a hidden layer \textbf{h} internally that
specifies a code used to represent the input. The network can be viewed as being
composed of two parts: an encoder function $\textbf{h} = f(\textbf{x})$ and a
decoder producing the reconstruction $\textbf{r} = g(\textbf{h})$. The general
structure of autoencoders can be seen in Figure \ref{fig:autoencoder}.

If an autoencoder is able to learn to set $g(f(\textbf{x}))=\textbf{x}$
anywhere, it is not particularly useful. Rather, autoencoders are designed to be
unable to learn how to copy perfectly. Usually they are restricted in ways that
only approximately allow them to copy. Additionally, it can handle only input
that is similiar to the training data. Since the model is forced to prioritize
which input features are to be copied, it often learns useful data characteristics. 

For decades, the idea of autoencoders has been part of neural networks' history.
Autoencoders have traditionally been used to reduce dimensionality or to learn
features. Recently, connections between latent variable models and autoencoders
brought up autoencoders as a cutting edge technology for generative modeling.
Autoencoders can be considered as a special case of feedforward networks and can
be trained with the same techniques, typically minibatch gradient descent after
gradients calculated by back-propagation. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{/1_introduction/autoencoder}
    \caption{An autoencoder's general structure, mapping an input $\textbf{x}$
    to an output (reconstruction) $\textbf{r}$ by an internal representation or
    code $\textbf{h}$. There are two components in the autoencoder: the encoder
    $f$ (mapping $\textbf{x}$ to $\textbf{h}$) and $g$ (mapping $\textbf{h}$ to
    $\textbf{r}$).}
    \label{fig:autoencoder}
\end{figure}


\subsection{Undercomplete Autoencoders}
Copying the input to the output may sound useless, but we are usually not
concerned about the decoder's output. Instead, by training the autoencoder we
try to achieve that $ \textbf{h}$ is taking on useful features, as a result of
copying the input to the output \cite{vincent2010stacked}.

To obtain useful features from the autoencoder we define $\textbf{h}$ with a smaller size
than $\textbf{x}$. \textbf{Undercomplete} autoencoders have a code dimension
which is smaller than the input dimension. Learning an undercomplete
representation causes the autoencoder to capture the training data's most
important features. 

We can describe this learning process as minimizing a loss function
\begin{equation}
    L(\textbf{x}, g(f(\textbf{x}))),
\end{equation}
where L is a loss function that penalizes $g(f(\textbf{x}))$ as being dissimilar
to $\textbf{x}$, such as the mean squared error. 

An undercomplete autoencoder learns to span the same subspace as principal
component analysis (PCA) when the decoder is linear and $L$ is the mean squared
error. In this case, as a side effect the autoencoder learned the prinicipal
suspace of the training data while performing training.

Thus, autoencoders with nonlinear encoder functions $f$ and nonlinear decoder
functions $g$ can learn to generalize PCA more powerfully. However, if too much
capacity is permitted for the encoder and decoder, the autoencoder might learn to
perform the copying task without extracting useful features from the data. 

\subsection{Regularized Autoencoders}
An akin problem happens when the hidden code has dimensions equal to the input,
or dimensions larger than the input. Under those circumstances, even a linear
encoder and a linear decoder learns how to replicate the input to the output
without learning anything meaningful about the distribution of the data  
\cite{makhzani2015adversarial}.

Ideally, any autoencoder architecture could be successfully trained by choosing
the code dimension and the encoder and decoder capacity based on the complexity
of the distribution to be modeled. Regularized autoencoders support this
capability. Instead of restricting the capacity of the model by keeping the
encoder and decoder shallow and code size small, regularized autoencoders use a
loss function that advocates the model to have other aspects than to copy its
input to its output. These other aspects include sparsity of representation,
smallness of representation derivative, and robustness to noise or missing
inputs. 

In conjuction with the methods explained here, which are interpreted most
naturally as regularized autoencoders, almost any generative model with latent
variables and equipped with an inference procedure (for computing latent
representations given input) can be understood as a particular form of
autoencoder. Variational autoencoders are another generative modeling approach
that emphasizes this relation with autoencoders. Naturally, these models learn
high-capacity, overcomplete input encodings and do not neeed regularization to
be useful. Their encoding is naturally effective as the models have been trained
to approximately maximize the probability of training data rather than copying
the input to the output. 

\subsection{Stochastic Encoders and Decoders}
Autoencoders are feedworward networks. The same loss functions and output unit
types that are used for traditional feedforward networks can be applied for
autoencoders aswell.

A general strategy for the design of the output units and the feedforward
network loss function is to define the output distribution
$p(\textbf{y}|\textbf{x})$ and minimize the negative log-likelihood $-\log
p(\textbf{y}|\textbf{x})$ . In that case \textbf{y} is a vector of targets, such
as class labels \cite{werbos2002stochastic}.

We may think of the decoder as a conditional distribution \\
$p_{decoder}(\textbf{x}|\textbf{h})$ given a hidden code $\textbf{h}$. Then, by
minimizing 
\begin{equation}
    -\log p_{decoder}(\textbf{x}|\textbf{h})
\end{equation}
we can train the autoencoder. The exact shape of this loss function depends on
the shape of \textbf{p}. Additionally, we can generalize the notion of an
\textbf{encoding function} $f(\textbf{x})$ to an \textbf{encoding distribution}
$p_{encoder}(\textbf{h}|\textbf{x})$.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{/1_introduction/autoencoder_stochastic}
    \caption{The architecture of a stochastic autoencoder, in which both the
    encoder and the decoder are not simple functions. Their output can be seen
    as samples from a distribution, $p_{encoder}(\textbf{h}|\textbf{x})$ for the
    encoder and $p_{decoder}(\textbf{x}|\textbf{h})$ for the decoder.}
\end{figure}

\subsection{Variational Autoencoders} \label{subsec:varautoencoders}
Many generative models form on the idea of using a differentiable generator
network. The model uses a differentiable function that is typically defined by a
neural network to convert samples of latent variables \textbf{z} into samples
\textbf{x} or distributions over samples \textbf{x}. This model class combines
autoencoders that match the generator net with an inference net (encoder)
\cite{doersch2016tutorial}.

To create a sample from the model, the VAE first picks a sample \textbf{z}
from the code distribution $p_{model}(\textbf{z})$. Then the sample runs through
a differentiable generator network $g(\textbf{z})$. At last, \textbf{x} is
sampled from a distribution $ p_{model}(\textbf{x};g(\textbf{z})) =
p_{model}(\textbf{x}|\textbf{z})$. During training, the encoder
$q(\textbf{z}|\textbf{x})$ is used to obtain \textbf{z}, and
$p_{model}(\textbf{x}|\textbf{z})$ is then viewed as decoder network. The
architectural overview of an variational autoencoder is visualized in Figure
\ref{fig:autoencoder_architecture}.

The key observation behind variational autoencoders is that they can be trained by
maximizing the variational lower bound $\mathcal{L}(q)$ associated with data
point \textbf{x}:

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{/2_background/variational_autoencoder_architecture}
    \caption{The architectural overview of a variational autoencoder. On the
    left side is the encoder which is typically a convolutional neural network.
    In the middle we see the standard deviation and the mean of the distribution
    $q(z|x)$ which are optimized during training. Following with the sampled
    latent vector z and the decoder $p_{model}(x|z)$ containing the generator
    network $g(z)$.} 
	\label{fig:autoencoder_architecture}
\end{figure}

\begin{flalign}
    \mathcal{L}(q)  &= \mathds{E}_{z \sim q(\textbf{z}|\textbf{x})} \log p_{model}(\textbf{z}, \textbf{x}) + \mathcal{H}(q(\textbf{z}|\textbf{x})) \label{vae:1} && \\
                    &= \mathds{E}_{z \sim q(\textbf{z}|\textbf{x})} \log 
                    p_{model}(\textbf{x}|\textbf{z}) -
                    D_{KL}(q(\textbf{z}|\textbf{x})||p_{model}(\textbf{z})) \label{vae:2} && \\
                    &\leq \log p_{model}(\textbf{x}).
\end{flalign}

In equation \ref{vae:1}, the first term outlines the expected value of the joint
log-likelihood of the visible and hidden variables with respect to the
probabilty distribution $q(\textbf{z}|\textbf{x})$ (approximate posteriror). The
second term defines the entropy of the approximate posterior. The entropy term
encourages the variational posterior to place high probabilty mass on many
\textbf{z} values that could have generated \textbf{x}, rather than collapsing
to a single point estimate of the most likely value.

In equation \ref{vae:2} we describe the first term as the reconstruction
log-likelihood used in other autoencoders. The second term makes the posterior
distribution $q(\textbf{z}|\textbf{x})$ and the model prior
$p_{model}(\textbf{z})$ distribution approach each other. This is accomplished with the
Kullback-Leibler divergence which is a measure of how one probability
distribution is different from a second.

The main intuition behind the variational autoencoder is to train an encoder that
produces the parameters of $q$. As long as $\textbf{z}$ is a continuous
variable, it is possible to back-propagate through the samples (by using the
reparameterization trick) of $\textbf{z}$ drawn from $q(\textbf{z}|\textbf{x}) =
q(\textbf{z};f(\textbf{x};\boldsymbol\theta))$ to obtain a gradient with respect
to $\boldsymbol\theta$, where $\boldsymbol\theta$ defines the model parameters
of the VAE.

VAE achieves excellent results and is one of the state-of-the-art generative
modeling approaches. The main disadvantage is that samples from variational
autoencoders trained on images are likely to be a little bit blurry. It is not
yet known why this anomaly happens. One possibility is that the blurriness is an
instrinsic effect of maximum likelihood \cite{Goodfellow-et-al-2016}.

\subsubsection{Reparameterization trick}
Traditional neural networks execute a deterministic conversion of some input
variables \textbf{z}. When using VAE or other generative models, it is necessary
that neural networks implement a stochastic conversion of \textbf{z}.
However, it is not possible to backpropagate through a stochastic node. One
simple solution is to augment the neural network with extra inputs
$\boldsymbol\epsilon$ that are sampled from a simple probability distribution.
The neural network can then continue to perfrom deterministic computation
internally like calculating gradients for training using back-propagation
\cite{kingma2015variational}.

As an example let us look at the operation of generating a sample from a VAE.
During training the VAE model draws a sample \textbf{z} from
$q(\textbf{z}|\textbf{x})$, which might be a Gaussian distribution with mean
$\mu$ and variance $\sigma^2$.

\begin{equation}
    z \sim \mathcal{N}(\mu, \sigma^2).
\end{equation}

Because an individual sample of z is not created by a function, but rather by a
sampling process whose output changes every time we query it, it may seem
counterintuitive to take the derivatives of $\vect{z}$ with respect to the parameters of
its distribution, $\mu$ and $\sigma^2$. However, we can rewrite the sampling
process as transforming an underlying random value $\epsilon \sim
\mathcal{N}(\epsilon; 0, 1)$ to obtain a sample from the desired distribution:

\begin{equation}
    z = \mu + \sigma * \epsilon.
\end{equation}

Now it is possible to back-propagate through the sampling operation, by
regarding it as a deterministic operation with an extra input $\epsilon$.
Figure \ref{reparam_trick} visualizes the reparameterization trick.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{/2_background/reparam_trick}
    \caption{On the left hand side the original form is displayed. It is not
    possible to back propagate through a stochastic node. On the right side the
    reparameterised form is displayed with a stochastic node $\epsilon$ giving
    the opportunity to train parameters $\mu$ and $\sigma$ while
    back-propagation.} 
	\label{reparam_trick}
\end{figure}
